---
title: 各种损失函数深入解析
sitemap: true
categories: 深度学习
date: 2018-10-28 13:14:35
tags:
- 深度学习
---

# 常用损失函数及其形式

| 损失函数 | 形式 |
| --- | --- |
| 绝对值损失(L1损失) |   |
| 平方损失(L2损失) |   |
| 交叉熵损失 |   |

# 各个损失函数详细解析

## 绝对值损失

别名 $L_1$ 损失

## 平方损失:
平方损失的别名是 $L_2$ 损失

平方损失函数是由线性模型引出的. 对于最简单的线性模型, 可以用房屋面积和房屋价格来举例子, 假设我们已经知道了一些面积和价格的数据:

![](https://wx4.sinaimg.cn/mw690/d7b90c85ly1fvp6z84393j20bp06kaaa.jpg)

将其描绘出来如下图所示:

![](https://wx3.sinaimg.cn/mw690/d7b90c85ly1fvp6z83ngnj20gw0bxgll.jpg)

那么, 如果新来了一个面积, 我们能否根据已有的数据来预测它的价格, 这就是线性回归问题. 我们利用一条直线来拟合这些数据, 从而可以得到预测的价格, 如下图所示:

![](https://wx3.sinaimg.cn/mw690/d7b90c85ly1fvp6z83ygvj20gv0bj74d.jpg)

将这种最简单的线性回归一般化, 使其成为具有多个变量的线性模型, 就可以用向量的形式来表达, 如下所示:

$$h_\theta (x) = \theta ^Tx$$

对于上面的公式, 我们就可以求出多个不同的 $\theta$, 来得到不同的模型, 但是我们需要知道到底哪些模型是好的, 哪些是不好的, 因此, 就需要引入了评价机制来判断当前的参数 $\theta$ 是好还是坏, 这就引出了平方误差损失函数, 如下所示:

$$J(\theta) = \frac{1}{2} \sum_{i=1}^{m}{(h_\theta(x^{(i)}) - y^{(i)})^2}$$

这个公式本身非常好理解, 就是希望我们当前模型的参数 $\theta$ 可以让模型的输出结果与真实结果无限逼近. 但是问题是:

**为什么是平方形式? 对此,数学解释如下:**

一句话说明: **平方损失函数就是对theta的极大似然估计**

首先, 预测结果和真实结果之间肯定是有误差的, 我们假设这个误差是 $\epsilon ^{(i)}$ , 那么就有如下公式:

$$y^{(i)} = \theta ^T x^{(i)} + \epsilon ^{(i)}$$

而一般在实际使用中, 我们的训练数据是海量的, 根据中心极限定力, 我们可以假定误差满足均值为0, 方差为 $\sigma ^2$ 的正态分布, 即 $\epsilon^{(i)} \sim N(0, \sigma ^2)$ :

$$p(\epsilon^{(i)}) = \frac{1}{\sqrt {2 \pi } \sigma }exp(-\frac{(\epsilon^{(i)})^2}{2 \sigma ^2})$$

 这也就是说:

 $$p(y^{(i)} | x^{(i)};\theta) = \frac{1}{\sqrt {2 \pi } \sigma }exp(-\frac{(y^{(i)} - \theta ^T x^{(i)})^2}{2 \sigma ^2})$$

 $p(y^{(i)} | x^{(i)};\theta)$ 代表在给定的 $x^{(i)}$ 和参数 $\theta$ 下, $y^{(i)}$的分布概率, 这可以看做是在给定的 $\theta$　一个关于 $y$ 和$x$ 的函数. 与之相对的,我们也可以将其视为是关于参数 $\theta$ 的函数,如下所示:

 $$L(\theta) = L(\theta ; X, \vec y) = p(\vec y | X; \theta)$$

注意到, $\epsilon^{(i)} , y^{(i)} , x^{(i)}$ 都是独立同分布的, 因此, 根据极大似然定理, 我们希望下面的式子能够取得最大值(也就是在给定数据的情况下, 我们希望找到一组参数 $\theta$ , 来使这些数据出现的概率最大, 也就是概率积最大)

$$ L(\theta) = \prod_{i=1}^{m}{p(y^{(i)} | x{(i)} ; \theta)} = \prod_{i=1}^{m}{\frac{1}{\sqrt{2\pi} \sigma} exp\Big( -\frac{ (y^{(i)} - \theta ^T x^{(i)})^2}{2\sigma ^2} \Big)} $$

为方便计算, 对上式取对数, 可得:

$$ l(\theta) = log L(\theta) = log\prod_{i=1}^{m}{\frac{1}{\sqrt{2\pi} \sigma} exp\Big( -\frac{ (y^{(i)} - \theta ^T x^{(i)})^2}{2\sigma ^2} \Big)}$$
$$= \sum_{i=1}^{m} log \frac{1}{sqrt{2\pi} \sigma} exp\Big( -\frac{(y^{(i)} - \theta ^T x^{(i)})^2}{2\sigma ^2} \Big) = mlog\frac{1}{\sqrt{2\pi} \sigma} - \frac{1}{\sigma ^2}\times \frac{1}{2}\sum_{i=1}^{m}(y^{(i)} - \theta ^T x^{(i)})^2$$

为了让上面的式子取值最大, 那我们就只需要令下面的式子取值最小即可:

$$\frac{1}{2} \sum_{i=1}^{m} ( y^{(i)} - \theta ^T x^{(i)}) ^2$$

上面的形式恰好就是我们的平方误差损失函数(通常还需要对上面的损失函数做归一化, 也就是乘上 $\frac{1}{m}$ ), 这也是平方误差损失函数的来源. (但实际上, 要知道, 基于概率假设来说, 不一定非要是平方项, 另外, 无需在意 $\sigma$ 的具体值是什么)


## softmax 交叉熵损失

$$y_i = softmax(z_j) = \frac{e^{z_j}}{\sum_j e^{z_j}}$$

$$E(t,y) = -\sum_j t_j log y_j$$

上式中, $t$ 和 $y$ 分别表示神经网络的真实标签和预测输出, 第一个公式代表 softmax 激活函数.


## binary 交叉熵损失

首先定义符号说明:
- $p^{(i)}$: 第i个样本类别为1的真实概率(如第i个样本真实类别为1, 则概率为1, 否则为0)
- $o^{(i)}$: 第i个样本预测类别为1的概率
- $p_k^{(i)}$: 第i个样本类别为k的真实概率(如第i个样本真实类别为k, 则概率为1, 否则为0)
- $o_k^{(i)}$: 第i个样本预测类别为k的概率


面对二分类问题, 损失函数形式为:

$$J(W,b) = -\Big[\frac{1}{m} \sum_{i=1}{m}\big(y^{(i)}logo^{(i)} + (1-y^{(i)})log(1-o^{(i)})  \big) \Big]$$

面对多分类问题, 损失函数形式为:
$$J(W,b) = -\Big[\frac{1}{m} \sum_{i=1}^{m} \sum_{k=1}^{n} y_k^{(i)} log o_k^{(i)}  \Big]$$

交叉熵衡量了两个分布之间的差异性, 当概率相等时, 交叉熵最大, 则损失函数达到最小(因为加了负号)

# 损失函数之间的区别和联系

## 为什么分类问题要使用交叉熵损失而不用平方损失?
