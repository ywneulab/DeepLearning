---
title: 反向传播算法完整推导
sitemap: true
categories: 深度学习
date: 2018-09-28 16:35:27
tags:
- 深度学习
---


# 链式法则


# 神经网络计算过程



**对于神经网络中的单个神经元来说**,

![](https://wx1.sinaimg.cn/mw690/d7b90c85ly1fvqptyzt2aj206a05h3yg.jpg)


若输入信号为向量 $\vec x=(x_1, x_2, x_3, x_4, x_5)$ , 该层的权重为 $\vec w = (w_1, w_2, w_3, w_4, w_5)$, 偏置项为 $b$ , 那么该层的输出就为(其中$f$为激活函数):

$$ y= f(\sum_{i=1}^{n}w_i x_i +b)$$

化简成向量形式($\vec w , \vec x$均为列向量)为:

$$ y = f(\vec w ^T \vec x +b)$$


对于多层网络来说, 如下图所示

![](https://wx2.sinaimg.cn/mw690/d7b90c85ly1fvqcl7rwfcj209505omx9.jpg)

第一层为输入层, 神经元数量对应原始数据维数, 这一层不对数据进行输出, 直接输出

第二层为隐藏层, 可以有多个隐藏层, 每层神经元数量为中间特征维数(一般自定), 每层都具有一个权重矩阵, 将输入信号与权重矩阵做点乘运算, 加上偏置量以后按激活函数输出

第三层为输出层, 同样有一个权重矩阵, 若用于分类, 则神经元数量等于要分类的类别数

如果激活函数使用sigmoid函数, 则第二层和第三层的输出分别为(第一层输出为原始数据):

# 符号说明:
- $x$ : 向量 $x$
- $x_i$ : 向量 $x$ 的第 $i$ 项
- $x^{(i)}$ : 第 $i$ 个样本向量
- $x_j^{(i)} : 第 $i$ 个样本向量中的 第 $j$ 项

# 几个重要结论

| 条件说明 |  说明及问题 | 结论 |
| --- | --- | --- |
| 给定如下线性映射函数: <br> $u = W x$ <br> 其中 $x$ 是 $n$ 维向量, $W$ 是 $m\times n$ 的矩阵, $u$ 是 $m$维向量, <br> | 假设存在函数 $f(u)$ , 试求 $\nabla _w f$ 及 $\nabla _x f$  | 因为 $w_{ij}$ 只与 $u_i, x_j$ 有关(画出矩阵相乘示意图即可), 所以有: <br><br> $\frac{\partial f}{\partial w_{ij}} = \sum_{k=1}^{m}  \frac{\partial f}{\partial u_k} \frac{\partial u_k}{\partial w_{ij}} = \sum_{k=1}^{m} \Big( \frac{\partial f}{\partial u_k} \frac{\partial \sum_{l=1}^{n} (w_{kl} x_l)}{\partial w_{ij}}\Big) = \frac{\partial f}{\partial u_i} \frac{\partial \sum_{l=1}^{n}(w_{il} x_l)} {\partial w_{ij}} = \frac {\partial f}{\partial u_i} x_j$ <br><br> 上式写成矩阵形式为: $\nabla _W f = (\nabla _u f) x^T$ <br><br> 因为 $x_i$与每一个 $u_k$ 都有关, 所以可得:  <br><br> $\frac{\partial f}{x_i} = \sum_{k=1}^{m} \frac{\partial f}{\partial u_k} \frac{\partial u_k}{\partial x_i} = \sum_{k=1}^{m} \Big( \frac{\partial f}{u_k} \frac{\partial \sum_{l=1}^{n} w_{kl} x_l}{\partial x_i} \Big)= \sum_{k=1}^{m} \Big( \frac{\partial f}{u_k} w_{ki} \Big) = [w_{1i}, w_{2i},..., w_{mi}]\left[ \begin{matrix} \frac{\partial f}{u_1} \\ \frac{\partial f}{u_2} \\ ... \\ \frac{\partial f}{u_2}  \end{matrix} \right]$ <br><br> 上式写成矩阵形式为: $\nabla _x f = W^T \nabla _u f$|
|  给定如下向量到向量的映射: <br><br> $z=g(u)$ <br><br> 写成分量形式为: <br><br> $z_i = g(u_i)$ <br><br> 在这里, 每个 $z_i$ 只和 $x_i$ 有关, 且每个分量采用了相同的映射函数$g$|  假设存在函数 $f(z)$, 试求 $\nabla _u f$ |  $\frac{\partial f}{\partial u_i} = \frac{\partial f}{\partial z_i} \frac{\partial z_i}{\partial u_i} = \frac{\partial f}{\partial z_i} g'(u_i)$ <br><br> $\nabla _u f = \nabla _z f \odot g'(u)$|
| 给定下面的复合函数 |   |   |


#

| 推导过程说明 |  详细推导  |  简洁推导  |
| --- | --- | --- |
|   |   |   |


# 反向传播中的一些特殊环节

## RuLe激活函数的导数

ReLU 激活函数的公式定义如下:

$$ReLu(x) = \begin{cases} x, & x > 0 \\ 0, & x\le 0 \end{cases}$$

可以看出, RuLu函数在 $x=0$ 处是不可微的, 为了解决这个问题, 在深度学习框架中, 往往会将其在 $x=0$ 处的导数置为0, 如下所示:

$$ReLu'(x) = \begin{cases} 1, & x > 0 \\ 0, & x\le 0 \end{cases}$$

## Pooling池化层的反向梯度传播

CNN网络中另外一个不可导的环节就是Pooling层的池化操作, 因为Pooling操作会使得feature map的尺寸发生变化. 解决这个问题的方法就是把一个该层某个位置的梯度反向传播到前一层所有与这个位置相关联的位置. 这是需要 **保证传递的梯度总和不变.**

### max pooling

![](https://wx4.sinaimg.cn/mw690/d7b90c85ly1fw9pql38u3j20e205cjrl.jpg)

![](https://wx2.sinaimg.cn/mw690/d7b90c85ly1fw9pugvbazj20e2058mxh.jpg)


### mean pooling

![](https://wx4.sinaimg.cn/mw690/d7b90c85ly1fw9pud49frj20dl04nglt.jpg)

![](https://wx2.sinaimg.cn/mw690/d7b90c85ly1fw9pund35ej20e7060aae.jpg)


# Reference

https://zhuanlan.zhihu.com/p/39195266

https://zhuanlan.zhihu.com/pytlab
