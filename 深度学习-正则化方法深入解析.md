---
title: 正则化方法深入解析
sitemap: true
categories: 深度学习
date: 2018-10-28 13:16:02
tags:
- 深度学习
- 知识点梳理
---

# L1 正则

L1范数为:

$$\|w\|_1 = |w_1| + |w_2| + ... + |w_n|$$

L1正则项如下所示, 其中 $L_0$ 代表原始的不加正则项的损失函数, $L$ 代表加了正则项以后的损失函数, ~~$m$ 则代表训练batch的样本大小~~ :

$$L = L_0 + \lambda\|w\|_1 = L_0 + \lambda \sum_{w}|w| $$

将上式对参数 $w$ 求导如下(由于正则项与 $b$ 无关, 因此参数 $b$ 的导数不变):

$$\frac{\partial L}{\partial w} = \frac{\partial L_0}{\partial w} + \lambda sign(w)$$

上式中 $sign(w)$ 表示 $w$ 的符号, 当 $w>0$ 时, $sign(w)=1$ , 当 $w<0$ 时, $sign(w)=-1$, 为了实现方便, 我们特意规定, 当 $w=0$ 时,  $sign(w) = 0$ , 相当于去掉了正则项.

因此, 权重 $w$ 的更新表达式可如下表示:

$$w \to w' = w - \eta \frac{\partial L_0}{\partial w} - \eta \lambda sign(w)$$

# L2 正则(权重衰减/岭回归)

L2范数为:

$$\|w\|_1 = \sqrt {w_1^2 + w_2^2 + ... + w_n^2 }$$

L2正则项如下所示, 其中 $L_0$ 代表原始的不加正则项的损失函数, $L$ 代表加了正则项以后的损失函数, ~~式中的系数 $\frac{1}{2}$ 主要是为了消去求导后产生的常数 $2$, 方便表示 (因为可以根据 $\lambda$ 的值来替代这些常数)~~:

$$L = L_0 + \lambda\|w\|^2_2 =L_0 + \lambda \sum_{w}w^2 $$

将上式对参数 $w$ 求导如下:

$$\frac{\partial L}{\partial w} = \frac{\partial L_0}{\partial w} + 2\lambda w$$

则, 权重 $w$ 的更新表达式可如下表示:

$$w \to w' = w - \eta \frac{\partial L_0}{\partial w} - \eta 2\lambda w$$

由于, $\eta, \lambda, m$ 三个值都是正的, 因此, 加上 $L2$ 正则化以后, 权重整体减小了, 这也是"权重衰减"的由来.

# L1 正则和 L2 正则的特点是什么? 各有什么优势?

二者共同的特点都是能够防止过拟合问题.

L1 的优点: 能够获得更加稀疏的模型.
L1 的缺点: 加入 L1 后会使得目标函数在原点不可导, 需要做特殊处理

L2 的有点: 在任意位置都可导, 优化求解过程比较方便, 而且更加稳定
L2 的缺点: 无法获得真正的稀疏模型

**在实际应用过程中, 大部分情况下都是 L2 正则的效果更好, 因此推荐优先使用 L2 正则**

# L1 和 L2 的区别

1. L1 相对于 L2 能够产生更加稀疏的模型
2. L2 相比于 L1 对于离异值更敏感(因为平方的原因, L2 对于大数的乘法比对小数的惩罚大)
3. L1 和 L2 梯度下降速度不同: 前者梯度恒定, 并且接接近于 0 的时候会很快将参数更新成0, 后者在接近于0 时, 权重的更新速度放缓, 使得不那么容易更新为0 (这也解释了为什么 L1 具有稀疏性)
4. 二者解空间性状不同

下图是 L1 和 L2 对向量中值的分布的先验假设, L1 是蓝色的线, L2 是红色的线, 可以看出, L1 的分布对于极端值更能容忍.
![L1和L2的先验假设](https://wx1.sinaimg.cn/large/d7b90c85ly1g12m2q505xj20dc0dcmyk.jpg)

![](https://wx3.sinaimg.cn/large/d7b90c85ly1fvxnikras2j20em0byjsh.jpg)

# L1正则化使模型参数稀疏的原理是什么?

角度一: 函数图像

L1 在 0 处迅速下降到 0, L2 在 0 处会变得缓慢, 并不会直接更新为 0.

角度二: 函数叠加(梯度下降更新公式)

$$w \to w' = w - \eta \frac{\partial L_0}{\partial w} - \eta \lambda sign(w) \tag{L1}$$

$$w \to w' = w - \eta \frac{\partial L_0}{\partial w} - \eta 2\lambda \tag{L2}w$$

从以上的更新表达式我们可以看出, 当 $w$ 为正时, L1正则化会将更新后的 $w$ 变的再小一点, 而当 $w$ 为负时, L1正则化会将其变的更大一点---**因此L1的正则化效果就是让 $w$ 尽可能的向 $0$ 靠近, 即最终的 $w$ 参数矩阵会变的更加稀疏**

角度三: 贝叶斯先验, "百面机器学习"
角度四: 解空间性状, "百面机器学习"


# 为什么 L1 和 L2 分别对应拉普拉斯先验和高斯先验?

# 为什么权重矩阵稀疏可以防止过拟合?

可以从两个方面来理解:

1）特征选择(Feature Selection)：稀疏性可以实现特征的自动选择, 可以在进行预测时减少无用信息的干扰

大家对稀疏规则化趋之若鹜的一个关键原因在于它能实现特征的自动选择。一般来说，$x_i$ 的大部分元素（也就是特征）都是和最终的输出 $y_i$ 没有关系或者不提供任何信息的，在最小化目标函数的时候考虑 $x_i$ 这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的信息反而会被考虑，从而干扰了对正确 $y_i$ 的预测。稀疏规则化算子的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些没有信息的特征，也就是把这些特征对应的权重置为 0。

2）可解释性(Interpretability)：较稀疏的模型表示最终的预测结果只与个别关键特征有关, 这符合实际生活中的历史经验

另一个青睐于稀疏的理由是，模型更容易解释。例如患某种病的概率是y，然后我们收集到的数据x是1000维的，也就是我们需要寻找这1000种因素到底是怎么影响患上这种病的概率的。假设我们这个是个回归模型： $y=w_1 \times x_1+w_2 \times x_2+…+w_{1000} \times x_{1000}+b$ （当然了，为了让 $y$ 限定在 $[0,1]$ 的范围，一般还得加个Logistic函数）。通过学习，如果最后学习到的 $w$ 就只有很少的非零元素，例如只有 5 个非零的 $wi$ ，那么我们就有理由相信，这些对应的特征在患病分析上面提供的信息是巨大的，决策性的。也就是说，患不患这种病只和这5个因素有关，那医生就好分析多了。但如果 1000 个 $w_i$ 都非 0，医生面对这 1000 种因素，累觉不爱.

# 为何权重参数 $w$ 减小就可以防止过拟合?

直观解释:

更小的权值w，从某种意义上说，表示网络的复杂度更低，对数据的拟合刚刚好（这个法则也叫做奥卡姆剃刀），而在实际应用中，也验证了这一点，L2正则化的效果往往好于未经正则化的效果

"数学一点"的解释:

过拟合的时候，拟合函数的系数往往非常大，为什么？如下图所示，过拟合，就是拟合函数需要顾忌每一个点，最终形成的拟合函数波动很大。在某些很小的区间里，函数值的变化很剧烈。这就意味着函数在某些小区间里的导数值（绝对值）非常大，由于自变量值可大可小，所以只有系数足够大，才能保证导数值很大. 而正则化是通过约束参数的范数使其不要太大，所以可以在一定程度上减少过拟合情况。

![](https://wx1.sinaimg.cn/mw690/d7b90c85ly1fvu4qp7cdwj2067060jri.jpg)

# $L0$ 范式和 $L1$ 范式都能实现稀疏, 为什么不选择用 $L0$ 而要用 $L1$?

L0范数是指向量中非零元素的个数

一是因为L0范数很难优化求解（NP难问题），二是L1范数是L0范数的最优凸近似，而且它比L0范数要容易优化求解


# 为什么说 L2 正则可以优化计算?

**防止过拟合:**

最基本的好处是可以提高模型泛化能力, 防止过拟合

**优化计算:**

从优化或者数值计算的角度来说, L2正则化有利于提高模型训练速度, 加快计算

原因: https://www.cnblogs.com/callyblog/p/8094745.html

# 正则项前面的系数一般怎么设置?
通常做法是一开始将正则项系数 $\lambda$ 设置为 0, 然后先确定出一个比较好的 learning rate, 接着固定该 learning rate, 给 $lambda$ 一个初始值, 如 1.0. 然后根据验证集上的准确率, 将 $\lambda$ 增大或者缩小 10 倍, 这里增减 10 倍是粗调节, 当确定了 $\lambda$ 合适的数量级以后, 再进一步的细调节.

# Reference
[知乎: l1正则与l2正则的特点是什么，各有什么优势](https://www.zhihu.com/question/26485586)
[知乎: 机器学习中常常提到的正则化到底是什么意思](https://www.zhihu.com/question/20924039)
